### 实现简单线性回归pytorch

还没有使用nn.Module

```
import torch
import numpy as np
import matplotlib.pyplot as plt

# y=3x+0.8

x = torch.rand((500, 1))
y_ture = x * 3 + 0.8

w = torch.rand((1, 1), requires_grad=True)
b = torch.tensor(0, requires_grad=True,dtype=torch.float32)


learning=0.01



for i in range(2000):
    y_pre = torch.matmul(x, w) + b
    loss = (y_ture - y_pre).pow(2).mean()
    if w.grad !=None:
        w.grad.data.zero_()  # .data浅拷贝
    if  b.grad !=None:
        b.grad.data.zero_()
    loss.backward()
    w.data=w.data-learning*w.grad.data
    b.data=b.data-learning*b.grad.data
    if i%10==0:
        print("w,b,loss:",w.item(),b.item(),loss.item())


plt.plot(x.numpy().reshape(-1),y_ture.numpy().reshape(-1))

y_predict=torch.matmul(x,w)+b

plt.plot(x.numpy().reshape(-1),y_predict.detach().numpy().reshape(-1),color='g')
plt.show()

```

#### tensor.detach()

返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置

不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad
