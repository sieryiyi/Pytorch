### 一、实现简单线性回归

还没有使用nn.Module

```
import torch
import matplotlib.pyplot as plt

# y=3x+0.8

x = torch.rand((500, 1))
y_ture = x * 3 + 0.8

w = torch.rand((1, 1), requires_grad=True)
b = torch.tensor(0, requires_grad=True,dtype=torch.float32)

learning=0.01

for i in range(2000):
    y_pre = torch.matmul(x, w) + b
    loss = (y_ture - y_pre).pow(2).mean()
    if w.grad !=None:
        w.grad.data.zero_()  # .data浅拷贝
    if  b.grad !=None:
        b.grad.data.zero_()
    loss.backward()
    w.data=w.data-learning*w.grad.data
    b.data=b.data-learning*b.grad.data
    if i%10==0:
        print("w,b,loss:",w.item(),b.item(),loss.item())


plt.plot(x.numpy().reshape(-1),y_ture.numpy().reshape(-1))

y_predict=torch.matmul(x,w)+b

plt.plot(x.numpy().reshape(-1),y_predict.detach().numpy().reshape(-1),color='g')
plt.show()

```

#### tensor.detach()

    是深拷贝，而.data是浅拷贝

    返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置

    不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad


### 二、实现基础模型（pytorch-API）

#### 1、nn.module 线性层

1、继承父类方法

2、定义forward方法：完成一次前向计算过程

以下完成y=wx+b

``` 
from torch import nn

class Lr(nn.Module):
    def __init__(self):
        super(Lr,self).__init__()  # 继承父类的初始化函数
        self.linear=nn.Linear(1,1)
    def forward(self,x):   # 在外创建实例，传入x，自动调用forward方法
        out=self.linear(x)
        return out
        
```
```
L1=Lr()

predict_out=L1(x)  # 而不用写L1.forward(x)
```

#### 2、优化器类 optimizer

1、torch.optim.SGD(参数，学习率)    ---------- 随机梯度下降

2、torch.optim.Adam(参数，学习率)

```
举例：假设模型的实例化为model1

model.parameters()用来获取所有requires_grad=True的参数

optimizer=torch.optim.SGD(model.parameters(),Tr=0.001)  # 实例中存放了需要更新的参数

optimizer.zeros_grad()  # 梯度置零，注意这个语法！！！！！！！！一次性清零梯度

loss.backword()

optimizer.step()  # 更新参数

```

#### 3、损失函数

torch.nn.MSEloss()

```
model=Lr()

loss_func=nn.MSEloss()  # 损失函数实例化

optimizer=torch.optim.SGD(model.parameters(),Tr=0.001)  # 实例中存放了需要更新的参数

for i in range(100):

    predict=model(x_true)
    
    loss=loss_func(y_true,predict)

    optimizer.zeros_grad()  # 梯度置零，注意这个语法！！！！！！！！一次性清零梯度

    loss.backword()

    optimizer.step()  # 更新参数

```

#### 4、实现y=3x+0.8 模型训练和评估

```
import torch
import matplotlib.pyplot as plt
from torch import nn

class mylinear(nn.Module):
    def __init__(self):
        super(mylinear, self).__init__()
        self.l1 = nn.Linear(1, 1)

    def forward(self, x):
        out = self.l1(x)
        return out

x = torch.rand([500, 1])
y_true = 3 * x + 0.8
model = mylinear()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
lossfunc = nn.MSELoss()

# 循环，梯度下降，参数更新

for i in range(2000):
    predict = model(x)
    loss = lossfunc(predict, y_true)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(loss.item())
    
-------------------------将模型从训练模式转化为评估模式
model.eval()  # 转化为评估模式
predict = model(x)
predict = predict.data.numpy()
plt.scatter(x.data.numpy(), y_true.data.numpy(), c='r')
plt.plot(x.data.numpy(), predict)
plt.show()

```

### 三、不同的优化器

#### 1、梯度下降算法BGD

    每次循环送入所有样本

#### 2、随机梯度法SGD （对样本数量进行操作）

    每次选择一个样本进行训练

    缺点：取出的样本是噪声样本，优化方向反而错误

#### 3、小批量梯度下降mini-batch SGD （对样本数量进行操作）

    结合上两个，每次使用一小批数据

    需要自行选择学习率

#### 4、动量优化器 （对梯度进行操作）

    本次最终梯度  ▽w= 上次梯度*（1-α）+本次算出的梯度▽w* α
    
    使得梯度不会跳跃过快，平滑梯度的更新，让的梯度的摆动幅度变小

#### 5、AdaGrad （对学习率进行操作）自适应学习率优化器

#### 6、RMSProP （对学习率进行操作） 是AdaGrad 的改进

    学习率 η =上次梯度*（1-α）+(本次算出的梯度▽w)**2 * α
    
    最终学习率 η = 1/sqrt{η}

#### 7、Adam （同时对学习率和梯度进行改进）  是动量法+RMSProP的改进

    最终更新中，梯度等于动量法中的梯度，学习率等于RMSProP中的学习率


### 数据加载类

#### 1、数据集类

    1、继承Dataset类
    
    2、同时实现两个方法：__len__，__getitem__
    
    3、__add__方法，合并两个数据集（不需要自己实现）
    
```
    from torch.utils.data import Dataset
    from torch.utils.data import DataLoader
    import pandas as pd

    data_path = r"E:\python222\MackeyGlass_t17.txt"  # 文件路径，后续自己加


    class MyDataset(Dataset):
        # 通常会
        # 1、随机打乱
        # 2、处理成batch
        # 3、数据预处理
        def __init__(self):
            self.lines = open(data_path).readline()
            # 标签文本分开

        def __len__(self):
            # 获取长度
            return len(self.lines)

        def __getitem__(self, index):
            cur_line = self.lines[index].strip()
            lable = cur_line[:4]
            content = cur_line[4:]
            return lable, content


    if __name__ == '__main__':
        my_dataset = MyDataset()
        data_loader=DataLoader(dataset=my_dataset,batch_size=2,shuffle=True)
```

#### DataLoader 2、数据加载器类

```
data_loader=DataLoader(dataset=my_dataset,batch_size=2,shuffle=True,,drop_last=True)
```

#### 3、自带数据集

torchversion

torchtext


