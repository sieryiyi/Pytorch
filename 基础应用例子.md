### 实现简单线性回归pytorch

还没有使用nn.Module

```
import torch
import matplotlib.pyplot as plt

# y=3x+0.8

x = torch.rand((500, 1))
y_ture = x * 3 + 0.8

w = torch.rand((1, 1), requires_grad=True)
b = torch.tensor(0, requires_grad=True,dtype=torch.float32)

learning=0.01

for i in range(2000):
    y_pre = torch.matmul(x, w) + b
    loss = (y_ture - y_pre).pow(2).mean()
    if w.grad !=None:
        w.grad.data.zero_()  # .data浅拷贝
    if  b.grad !=None:
        b.grad.data.zero_()
    loss.backward()
    w.data=w.data-learning*w.grad.data
    b.data=b.data-learning*b.grad.data
    if i%10==0:
        print("w,b,loss:",w.item(),b.item(),loss.item())


plt.plot(x.numpy().reshape(-1),y_ture.numpy().reshape(-1))

y_predict=torch.matmul(x,w)+b

plt.plot(x.numpy().reshape(-1),y_predict.detach().numpy().reshape(-1),color='g')
plt.show()

```

#### tensor.detach()

是深拷贝，而.data是浅拷贝

返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置

不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad


### 实现基础模型（pytorch-API）

#### nn.module 线性层

1、继承父类方法

2、定义forward方法：完成一次前向计算过程

以下完成y=wx+b

``` 
from torch import nn

class Lr(nn.Module):
    def __init__(self):
        super(Lr,self).__init__()  # 继承父类的初始化函数
        self.linear=nn.Linear(1,1)
    def forward(self,x):   # 在外创建实例，传入x，自动调用forward方法
        out=self.linear(x)
        return out
        
```
```
L1=Lr()

predict_out=L1(x)  # 而不用写L1.forward(x)
```

#### 优化器类 optimizer

1、torch.optim.SGD(参数，学习率)    ---------- 随机梯度下降

2、torch.optim.Adam(参数，学习率)

```
举例：假设模型的实例化为model1

model.parameters()用来获取所有requires_grad=True的参数

optimizer=torch.optim.SGD(model.parameters(),Tr=0.001)  # 实例中存放了需要更新的参数

optimizer.zeros_grad()  # 梯度置零，注意这个语法！！！！！！！！一次性清零梯度

loss.backword()

optimizer.step()  # 更新参数

```

#### 损失函数

torch.nn.MSEloss()

```
model=Lr()

loss_func=nn.MSEloss()  # 损失函数实例化

optimizer=torch.optim.SGD(model.parameters(),Tr=0.001)  # 实例中存放了需要更新的参数

for i in range(100):

    predict=model(x_true)
    
    loss=loss_func(y_true,predict)

    optimizer.zeros_grad()  # 梯度置零，注意这个语法！！！！！！！！一次性清零梯度

    loss.backword()

    optimizer.step()  # 更新参数

```

#### 实现y=3x+0.8 模型训练和评估

```
import torch
import matplotlib.pyplot as plt
from torch import nn

class mylinear(nn.Module):
    def __init__(self):
        super(mylinear, self).__init__()
        self.l1 = nn.Linear(1, 1)

    def forward(self, x):
        out = self.l1(x)
        return out

x = torch.rand([500, 1])
y_true = 3 * x + 0.8
model = mylinear()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
lossfunc = nn.MSELoss()

# 循环，梯度下降，参数更新

for i in range(2000):
    predict = model(x)
    loss = lossfunc(predict, y_true)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    print(loss.item())
    
-------------------------将模型从训练模式转化为评估模式
model.eval()  # 转化为评估模式
predict = model(x)
predict = predict.data.numpy()
plt.scatter(x.data.numpy(), y_true.data.numpy(), c='r')
plt.plot(x.data.numpy(), predict)
plt.show()

```

### 不同的优化器

#### 梯度下降算法BGD

每次循环送入所有样本

#### 随机梯度法SGD 

每次选择一个样本进行训练

缺点：取出的样本是噪声样本，优化方向反而错误

#### 小批量梯度下降mini-batch SGD

结合上两个，每次使用一小批数据

需要自行选择学习率

#### 动量优化器

本次最终梯度=上次梯度*（1-α）+本次算出的梯度* α

使得梯度不会跳跃过快，平滑梯度的更新，让的梯度的摆动幅度变小



